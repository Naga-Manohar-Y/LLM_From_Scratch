{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preparation and Sampling**\n",
        "## **Tokenization**"
      ],
      "metadata": {
        "id": "ExHSqTHMaj6K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0VNKSxDadVG",
        "outputId": "e1871bdc-6dda-480f-c6fa-73ed6b1f0a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 439742 characters\n"
          ]
        }
      ],
      "source": [
        "with open(\"Harry_Potter_Sorcerer's_Stone.txt\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(f\"Length of text: {len(raw_text)} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5GgnD0SfGkH",
        "outputId": "44b38484-24c1-4988-a897-3da8c673927b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om71ve-DfCGx",
        "outputId": "167a9c85-24d1-4bdb-e8aa-23e1a163e514"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W64sDGRegReX",
        "outputId": "45cc872e-f36f-4c5f-9475-8e85947dc9a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "removing spaces or not.\n",
        "- here as we training on normal text there is no specific importance of space. so we can remove it."
      ],
      "metadata": {
        "id": "lZXby7Ecg-iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, -world. Is this-- a test?\"\n",
        "\n",
        "result = re.split(r'([-,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVPzrX9Kg8DF",
        "outputId": "ef6642e1-4416-4bc2-b40e-453a76b1d2a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '-', 'world', '.', 'Is', 'this', '-', '-', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([-,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KruYWtoiiLTH",
        "outputId": "c6593fd4-001c-4e7d-883f-ab6df7cfe68b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Harry', 'Potter', 'and', 'the', 'Sorcerer', \"'\", 's', 'Stone', 'CHAPTER', 'ONE', 'THE', 'BOY', 'WHO', 'LIVED', 'Mr', '.', 'and', 'Mrs', '.', 'Dursley', ',', 'of', 'number', 'four', ',', 'Privet', 'Drive', ',', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', ',', 'thank', 'you', 'very', 'much', '.', 'They', 'were', 'the', 'last', 'people', 'you', \"'\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preprocessed))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLsEvHp4iYAM",
        "outputId": "9048765f-db2b-45d5-8ae0-3033a6aaa059"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "103826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Next, we convert the text tokens into token IDs that we can process via embedding layers later"
      ],
      "metadata": {
        "id": "B80WzJRNio6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7ikIi7hiofg",
        "outputId": "688cd38d-5941-459a-caea-c3ed2ccffa35"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- assigning token id's to sorted tokens"
      ],
      "metadata": {
        "id": "ebnj2NArjmC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer, token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "Mv41pT5ojYtL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i > 60:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw36iF5Pj4CL",
        "outputId": "4f9b7ff6-4156-4614-df15-de93ee9b48fe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('-', 6)\n",
            "('.', 7)\n",
            "('0', 8)\n",
            "('1', 9)\n",
            "('1473', 10)\n",
            "('1637', 11)\n",
            "('17', 12)\n",
            "('1709', 13)\n",
            "('1945', 14)\n",
            "('2', 15)\n",
            "('3', 16)\n",
            "('31', 17)\n",
            "('382', 18)\n",
            "('4', 19)\n",
            "('90', 20)\n",
            "(':', 21)\n",
            "(';', 22)\n",
            "('?', 23)\n",
            "('A', 24)\n",
            "('AAAAAAAAAARGH', 25)\n",
            "('AAAARGH', 26)\n",
            "('ALBUS', 27)\n",
            "('ALL', 28)\n",
            "('ALLEY', 29)\n",
            "('ALLOWED', 30)\n",
            "('AM', 31)\n",
            "('AND', 32)\n",
            "('ANYTHING', 33)\n",
            "('ARE', 34)\n",
            "('AT', 35)\n",
            "('Aaah', 36)\n",
            "('Aargh', 37)\n",
            "('Abbott', 38)\n",
            "('Abou', 39)\n",
            "('About', 40)\n",
            "('Absolutely', 41)\n",
            "('According', 42)\n",
            "('Adalbert', 43)\n",
            "('Add', 44)\n",
            "('Adrian', 45)\n",
            "('Africa', 46)\n",
            "('African', 47)\n",
            "('After', 48)\n",
            "('Against', 49)\n",
            "('Ages', 50)\n",
            "('Agrippa', 51)\n",
            "('Ah', 52)\n",
            "('Aha', 53)\n",
            "('Ahead', 54)\n",
            "('Ahem', 55)\n",
            "('Ahern', 56)\n",
            "('Alas', 57)\n",
            "('Alberic', 58)\n",
            "('Albus', 59)\n",
            "('Algie', 60)\n",
            "('Alicia', 61)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The encode function turns text into token IDs\n",
        "- The decode function turns token IDs back into text"
      ],
      "metadata": {
        "id": "nizTr0MKmvfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([-,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "Htv5JbP0ladu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)"
      ],
      "metadata": {
        "id": "E0gljlY-mu0M"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\" \"The Potters, that's right,\n",
        "that's what I heard yes, their son, Harry\" \"\"\"\n",
        "\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TmIaRn7nrnQ",
        "outputId": "ae4ed78b-db1b-427e-c081-bdec03fe075e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1233, 967, 5, 5995, 2, 5067, 4985, 5, 5995, 2, 5067, 6472, 640, 3500, 6648, 5, 5997, 5520, 5, 570, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "dBB636b7n75y",
        "outputId": "d805c353-f93c-4a98-ea5b-a86aeba6d647"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" The Potters, that\\' s right, that\\' s what I heard yes, their son, Harry\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, transformer-- a test?\"\n",
        "\n",
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "pb6YGjVgp01K",
        "outputId": "e5d8d47d-e692-4452-a037-e50ec7e55659"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'transformer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-65b206801776>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, transformer-- a test?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-d3228ee8d6ad>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         ]\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-d3228ee8d6ad>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         ]\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'transformer'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Handling unkown tokens"
      ],
      "metadata": {
        "id": "DbRZlBfFqYSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Some tokenizers use special tokens to help the LLM with additional context\n",
        "\n",
        "- Some of these special tokens are\n",
        "\n",
        "- [BOS] (beginning of sequence) marks the beginning of text\n",
        "- [EOS] (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two different books, and so on)\n",
        "- [PAD] (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length)\n",
        "- [UNK] to represent words that are not included in the vocabulary\n",
        "\n",
        "- Note that GPT-2 does not need any of these tokens mentioned above but only uses an <|endoftext|> token to reduce complexity\n",
        "\n",
        "- The <|endoftext|> is analogous to the [EOS] token mentioned above\n",
        "\n",
        "- GPT also uses the <|endoftext|> for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens anyways, so it does not matter what these tokens are)\n",
        "\n",
        "- GPT-2 does not use an <UNK> token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units which we will discuss in a later section\n",
        "\n",
        "- We use the <|endoftext|> tokens between two independent sources of text:"
      ],
      "metadata": {
        "id": "clYdcpM5re_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------\n",
        "- To deal with such cases, we can add special tokens like \"<|unk|>\" to the vocabulary to represent unknown words\n",
        "- Since we are already extending the vocabulary, let's add another token called \"<|endoftext|>\" which is used in GPT-2 training to denote the end of a text (and it's also used between concatenated text, like if our training datasets consists of multiple articles, books, etc.)"
      ],
      "metadata": {
        "id": "1APGzLlhsCDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])"
      ],
      "metadata": {
        "id": "Nyelu30bqeKi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "len(vocab.items())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqyO9NCnsOUG",
        "outputId": "b2162726-8d2b-471b-af1b-a6f47b4a7b9c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6669"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScW8BKwcsQUM",
        "outputId": "de60e50d-0d1e-4c76-86dd-5220124370aa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('zoom', 6664)\n",
            "('zoomed', 6665)\n",
            "('zooming', 6666)\n",
            "('<|endoftext|>', 6667)\n",
            "('<|unk|>', 6668)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([-,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int\n",
        "            else \"<|unk|>\" for item in preprocessed\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "7ai_fxpksTM6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, transformer?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPi_Y1jms6D9",
        "outputId": "7fdfd725-010a-42de-b140-47a9d39b4421"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, transformer? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30OEomEGs-c8",
        "outputId": "34acaa06-095c-44fc-f79c-fd8f08f9dd02"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[589, 5, 6668, 23, 6667, 650, 5996, 6668, 6668, 4390, 5996, 4485, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WiYquw1_tRat",
        "outputId": "a066b90f-1160-433b-d344-d7e295f0097e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, <|unk|>? <|endoftext|> In the <|unk|> <|unk|> of the palace.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Byte Pair Encoding"
      ],
      "metadata": {
        "id": "mWHrncQLujv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We implemented a simple tokenization scheme in the previous sections for illustration purposes.\n",
        "\n",
        "- This section covers a more sophisticated tokenization scheme based on a concept called byte pair encoding (BPE).\n",
        "\n",
        "- The BPE tokenizer covered in this section was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.\n",
        "\n",
        "- Since implementing BPE can be relatively complicated, we will use an existing Python open-source library called tiktoken (https://github.com/openai/tiktoken).\n",
        "\n",
        "- This library implements the BPE algorithm very efficiently based on source code in Rust."
      ],
      "metadata": {
        "id": "UI2xp5MrE4Xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **word based tokenization** but raises out of vocabulary problem\n",
        "- **character level tokenization** solves out of vocbulary error as it has only 256 characters. But the meaning associated with thr words is completely lost. Also, the tokenized sequence is much longer than the initial raw text.\n",
        "- **subword tokenization**\n",
        "- do not split frequently used words into smaller subwords\n",
        "- split the rare words into smaller meaningful words\n",
        "- It helps the model learn that different words with. same root word as 'token', 'tokens', and 'tokenizing' are similar in meaning.\n",
        "- It also helps the model learn that 'tokenization' and 'modernization' are made up of different root words but have the same suffix 'ization' and are useed in same syntactic situations."
      ],
      "metadata": {
        "id": "2AkMiJQCF71y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip3 install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPoGuJ1AujSR",
        "outputId": "0b20c861-0b2c-45fd-def5-fbea22c2f2d6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m1.0/1.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWjrAgCCL3Qz",
        "outputId": "c406005b-ee18-4c31-faf8-24547040eecd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "lSTPmHLhL-sd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vFmiCFXMGnr",
        "outputId": "852d129a-8596-41f2-ed7c-d4970fa9ea38"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above prints the following token IDs:\n",
        "\n",
        "We can then convert the token IDs back into text using the decode method, similar to our SimpleTokenizerV2 earlier:"
      ],
      "metadata": {
        "id": "5afzIt4gMlXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gkv4LrJMZTg",
        "outputId": "455659d7-a36e-4c83-a938-52db797dff6c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make two noteworthy observations based on the token IDs and decoded text above.\n",
        "\n",
        "- First, the <|endoftext|> token is assigned a relatively large token ID, namely,\n",
        "\n",
        "- In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, has a total vocabulary size of 50,257, with <|endoftext|> being assigned the largest token ID.\n",
        "\n",
        "- Second, the BPE tokenizer above encodes and decodes unknown words, such as \"someunknownPlace\" correctly.\n",
        "\n",
        "- The BPE tokenizer can handle any unknown word. How does it achieve this without using <|unk|> tokens?\n",
        "\n",
        "- The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary into smaller subword units or even individual characters.\n",
        "\n",
        "- The enables it to handle out-ofvocabulary words.\n",
        "\n",
        "- So, thanks to the BPE algorithm, if the tokenizer encounters an unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or characters"
      ],
      "metadata": {
        "id": "WVEsA5KBNC6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "integers = tokenizer.encode(\"Akwirw ier\")\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_NcuHs2NMEN",
        "outputId": "c9f5b360-c95f-4d75-c973-1208de38e8bf"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n",
            "Akwirw ier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Sampling with a sliding window"
      ],
      "metadata": {
        "id": "Kvc96HgXUHWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:\n",
        "\n",
        "- Auto-regressive and unsupervised learning"
      ],
      "metadata": {
        "id": "7FjP5fFmVFX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Harry_Potter_Sorcerer's_Stone.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehZat8nSUN9H",
        "outputId": "8044a7f0-392b-451d-aeca-bddbb50ee7b7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "116725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For each text chunk, we want the inputs and targets\n",
        "- Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right\n"
      ],
      "metadata": {
        "id": "QskJmEA2XKbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]\n"
      ],
      "metadata": {
        "id": "6tOXPPuPXSq6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUaj56pgWe2v",
        "outputId": "6546f15f-c9c9-4cb3-995c-2028155bf580"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [5875, 345, 845, 881]\n",
            "y:      [345, 845, 881, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEdDcoodXVWw",
        "outputId": "4c59c7e8-60cf-447d-c7a7-9af96a371c49"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5875] ----> 345\n",
            "[5875, 345] ----> 845\n",
            "[5875, 345, 845] ----> 881\n",
            "[5875, 345, 845, 881] ----> 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjPKPBStXtZ_",
        "outputId": "0f45bb02-d010-4e72-a419-35b7f00df840"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " thank ---->  you\n",
            " thank you ---->  very\n",
            " thank you very ---->  much\n",
            " thank you very much ----> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0HGdAq-vZFXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tdwM__eXygQ",
        "outputId": "6096e2d6-cecc-4203-dcc2-e673860885d0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The GPTDatasetV1 class is liting 2.5 is based on the PyTorch Dataset class.\n",
        "- It defines how individual rows are fetcched from the dataset.\n",
        "- Each row consists of a number of token IDs based on max_length assigned to the input_chunk tensor.\n"
      ],
      "metadata": {
        "id": "tPW4gu40Z_0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "9WnhdXaOZGpe"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "hNNhHk3ibFCR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Harry_Potter_Sorcerer's_Stone.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ],
      "metadata": {
        "id": "jAbi6DsZbpDU"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- stride -> how many words the next input should slide\n",
        "- max_length -> it is the input size\n",
        "- batch_size -> how many inputs (batch of inputs) it should process before updating it's parameters"
      ],
      "metadata": {
        "id": "b0nPAOSDdW0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDD6kxYqbf_l",
        "outputId": "f803cbff-a1c0-4797-805b-f2cedb4ca468"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[18308, 14179,   290,   262]]), tensor([[14179,   290,   262, 30467]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU6r8c7jcfni",
        "outputId": "c69af3aa-c370-4ef7-8d3f-120c337f82df"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[14179,   290,   262, 30467]]), tensor([[  290,   262, 30467,   338]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1xLT94Tc8C1",
        "outputId": "7f876323-3a6a-461a-d216-1aba032ca75b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[18308, 14179,   290,   262],\n",
            "        [30467,   338,  8026,   628],\n",
            "        [  198, 41481, 16329,   198],\n",
            "        [  198, 10970, 16494,    56],\n",
            "        [19494,   406,  3824,  1961],\n",
            "        [  198,   198,  5246,    13],\n",
            "        [  290,  9074,    13,   360],\n",
            "        [ 1834,  1636,    11,   286]])\n",
            "\n",
            "Targets:\n",
            " tensor([[14179,   290,   262, 30467],\n",
            "        [  338,  8026,   628,   198],\n",
            "        [41481, 16329,   198,   198],\n",
            "        [10970, 16494,    56, 19494],\n",
            "        [  406,  3824,  1961,   198],\n",
            "        [  198,  5246,    13,   290],\n",
            "        [ 9074,    13,   360,  1834],\n",
            "        [ 1636,    11,   286,  1271]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token Embeddings\n",
        "- words are represented as vectors to capture the semantic meaning.\n"
      ],
      "metadata": {
        "id": "Iadumf1WX2YR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Token Ids are converted into vector embeddings -> vector dimension X Vocabulary size\n",
        "- example: GPT-2 768 X 50257 (vec dim X vocab size)\n",
        "- wights are initialized randomly and updated in training."
      ],
      "metadata": {
        "id": "M3hRcGercuoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's illustrate how the token ID to embedding vector conversion works with a hands-on example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:"
      ],
      "metadata": {
        "id": "JujKPrqCfRI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ],
      "metadata": {
        "id": "sToo6E55X3Ox"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):\n",
        "\n",
        "- Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch, setting the random seed to 123 for reproducibility purposes:"
      ],
      "metadata": {
        "id": "MSzRkQfAfo0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "v5Kspo4Rfmwo"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The print statement in the code prints the embedding layer's underlying weight matrix:"
      ],
      "metadata": {
        "id": "jdN40ghYgAYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.weight)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3zQ1ayEfz1l",
        "outputId": "8bc07012-7ad6-4a56-e0a4-008ed4a0ce28"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can see that the weight matrix of the embedding layer contains small, random values. These values are optimized during LLM training as part of the LLM optimization itself, as we will see in upcoming chapters. Moreover, we can see that the weight matrix has six rows and three columns. There is one row for each of the six possible tokens in the vocabulary. And there is one column for each of the three embedding dimensions."
      ],
      "metadata": {
        "id": "N0Ps3A2HgT7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- After we instantiated the embedding layer, let's now apply it to a token ID to obtain the embedding vector:"
      ],
      "metadata": {
        "id": "5k302ua_gdWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjg5Q476f_Ms",
        "outputId": "c34d8ead-404d-445f-ee6f-b94c7c4f0888"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- If we compare the embedding vector for token ID 3 to the previous embedding matrix, we see that it is identical to the 4th row (Python starts with a zero index, so it's the row corresponding to index 3). In other words, the embedding layer is essentially a look-up operation that retrieves rows from the embedding layer's weight matrix via a token ID.\n",
        "\n",
        "- Previously, we have seen how to convert a single token ID into a three-dimensional embedding vector. Let's now apply that to all four input IDs we defined earlier (torch.tensor([2, 3, 5, 1])):"
      ],
      "metadata": {
        "id": "rLH-477dgmEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZD209KsEghye",
        "outputId": "2ee84f62-5ed5-4ab4-9d59-c14990159304"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Each row in this output matrix is obtained via a lookup operation from the embedding\n",
        "weight matrix"
      ],
      "metadata": {
        "id": "akmyNFLog9AW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POSITIONAL EMBEDDINGS (ENCODING WORD POSITIONS)**"
      ],
      "metadata": {
        "id": "MZtaZLNXivKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Absolute**: For each position in input sequence, a unique embedding is added to the token's embedding to convey it's exact location.\n",
        "- Suitable when fixed order of tokens is crucial, such as sequence generation.\n",
        "\n",
        "- **Relative**: The emphasis is on the relative position or distance between tokens. The model learns the relationships in terms of \"how far apart\" rather than at which exact position. **Advantage of this** : the model can generalize better to sequence of varying lengths, even if it has not seen such lengths during training.\n",
        "- Suitable for tasks like language modeling over long sequences, where the same phrase can appear in different parts of the sequence."
      ],
      "metadata": {
        "id": "dPQ7lBCYjwg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Open AI's GPT models use absolute positional embedding that are optimized during the training process. This optimization is part of the model training itself.\n",
        "\n"
      ],
      "metadata": {
        "id": "Wr4TXOdnqGrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Positional encoding enables LLM to understand the order and relationship between tokens, ensuring accurate and context aware predictions.\n",
        "- Both vector and positional embeddings are optimized during the training."
      ],
      "metadata": {
        "id": "yp-VEaLfo-ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Token embedding layer**"
      ],
      "metadata": {
        "id": "8LkJHoKprcoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "vusGT15SjwH6"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- inputs -> 8 text samples with 4 tokens\n",
        "- result -> 8 x 4 x 256 tensor"
      ],
      "metadata": {
        "id": "Dxwcql34rwBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ],
      "metadata": {
        "id": "uffLxceUiuyj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfcXZ6v3g5ED",
        "outputId": "c7f91816-361c-4b20-87cd-3a544f0d1816"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[18308, 14179,   290,   262],\n",
            "        [30467,   338,  8026,   628],\n",
            "        [  198, 41481, 16329,   198],\n",
            "        [  198, 10970, 16494,    56],\n",
            "        [19494,   406,  3824,  1961],\n",
            "        [  198,   198,  5246,    13],\n",
            "        [  290,  9074,    13,   360],\n",
            "        [ 1834,  1636,    11,   286]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KivXg2e7vOxU",
        "outputId": "cdce718c-d477-4634-ba58-75fc06acad3f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- generate the 4 positional embedding vectors from the positional embedding matrix"
      ],
      "metadata": {
        "id": "2GfXKkBAuNqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ],
      "metadata": {
        "id": "GNEZeZ1wsWn3"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLVKnGziutKG",
        "outputId": "6574d42e-1229-4ccd-9826-d5233cc22081"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We use [4, 256] positional embeddings because positional information is the same for all sequences in the batch. The batch dimension (8) is handled by broadcasting, avoiding redundancy and saving resources."
      ],
      "metadata": {
        "id": "28OLfkRhg3Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add the token and positional embeddings\n",
        "\n",
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kKQGnchu9gV",
        "outputId": "2327d60c-d917-4fd9-fa68-b96a311b3616"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLM Data Preprocessing**\n",
        "\n",
        "\n",
        "1.   Tokenization\n",
        "2.   Token Embeddings\n",
        "3. Positional Embeddings\n",
        "4. Input Embeddings = Token embeddings + positional embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "aomOhdxTMrFW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZqMXq84RM3c_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}